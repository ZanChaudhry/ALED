{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:\\\\Users\\\\Small\\\\OneDrive\\\\Documents\\\\Cleanlab Testing\\\\chest_xray\\\\train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALED Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import scipy.stats\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# IMPORTANT FOR REPEATABILITY\n",
    "SEED = 456\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ALED(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    # BaseEstimator provides get and set_params() functions and ClassifierMixin provides weighted accuracy\n",
    "\n",
    "    def __init__(self, random_state = 0): # random_state is default 0 for now, should be changed later!\n",
    "        # Note: I think the sklearn API would say that some fit() params [e.g., model, max_pca_components, max_pca_variance]\n",
    "        #   should be instantiation params and not fit() params\n",
    "\n",
    "        self.random_state = random_state\n",
    "\n",
    "    @staticmethod\n",
    "    def sum_until(in_list, threshold):\n",
    "        \"\"\"\n",
    "        Helper function for fit()\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for n in range(len(in_list)):\n",
    "            if count < threshold:\n",
    "                count += in_list[n]\n",
    "            else:\n",
    "                break\n",
    "        return count, n+1\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_likelihood(cov_matrix, mahalanobis_squared_dist):\n",
    "        \"\"\"\n",
    "        Helper function for fit()\n",
    "        \"\"\"\n",
    "        n = cov_matrix.shape[0]\n",
    "        sqrt_det = np.sqrt(np.linalg.det(cov_matrix))\n",
    "        prob_x_f = ( 1 / ( (2*np.pi)**(n/2) * sqrt_det ) ) * np.exp(-0.5*mahalanobis_squared_dist)\n",
    "        #if prob_x_f > 1: print(\"prob_x_f > 1; prob_x_f =\", prob_x_f)\n",
    "        return prob_x_f\n",
    "\n",
    "    def extract_conv_net_features(self, conv_net, dataset):\n",
    "        \"\"\"\n",
    "        Helper function for fit()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def map_conv_net_output(self):\n",
    "        \"\"\"\n",
    "        Map the last layer of the convolutional layer (or other last feature extraction layer)\n",
    "        so each sample ends up with 1 dimension of representative features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def extract_out_label(self, prediction_stats_row):\n",
    "        given_label = prediction_stats_row['given label (name)']\n",
    "        probabilities_row = np.array([prediction_stats_row[\"p(k = {} | x)\".format(class_i)] for class_i in self.classes_])\n",
    "        if (self.classes_[probabilities_row.argmax()] != given_label)\\\n",
    "           and (probabilities_row.max()/prediction_stats_row[\"p(k = {} | x)\".format(given_label)] > self.likelihood_ratio_threshold):\n",
    "            return self.classes_[probabilities_row.argmax()]\n",
    "        else:\n",
    "            return given_label\n",
    "\n",
    "    def fit_predict(self, model, dataset, device=None, max_pca_components=10, max_pca_variance=0.25, likelihood_ratio_threshold=2, prob_method=\"gaussian\", batch_size=100):\n",
    "        # sklearn would want us to separate into fit() and predict() (NR)\n",
    "        \"\"\"\n",
    "        Given a model and data, outputs predicted class for each sample based on ALED algorithm.\n",
    "        Creates a new model that aggregates input classification model features / feature maps at last layer before classification into a [p x 1] vector.\n",
    "        Then performs PCA to generate an [n x 1] vector of PCs, where n is specified either explicitly or by a desired level of explained variance. Then\n",
    "        the probability that a given sample belongs to the assigned class is assessed using Bayes' rule, and if the probability falls below the within\n",
    "        class threshold, then the sample is compared to the other class(es), and if the probability that it belongs in another class is higher than the\n",
    "        out of class threshold, then the sample is added to an output DataFrame containing suspect samples and their associated probabilities. This\n",
    "        function is dependent on the ability of the input classification model to extract salient features from the input data and thus should only be\n",
    "        applied if the model is achieving some threshold auc / accuracy.\n",
    "\n",
    "        ### Parameters\n",
    "        1. model : Pytorch model (torch.nn.Sequential)\n",
    "            -A classification model\n",
    "        2. dataset : Pytorch Dataset (torch.utils.data.Dataset, eventually...right now needs an ImageFolder)\n",
    "            -The dataset used to train model\n",
    "             note: the dataset is responsible for holding any transformation functions;\n",
    "             the transformations need to be the same transformations done for model evaluation; data augmentation should NOT be used!!!\n",
    "        x. device : str, or None\n",
    "            -Specifies device (e.g. 'cuda' GPU) to be used for processing\n",
    "        4. max_pca_components : int\n",
    "            -Maximum number of components used for PCA (10 by default);\n",
    "             determines\n",
    "        5. pca_variance : float\n",
    "            -Ratio of explained variance desired, if\n",
    "             using explained variance method (0.25 by\n",
    "             default)\n",
    "        6. wc_threshold : float\n",
    "            -Within class threshold to consider sample\n",
    "             within distribution of other class(es) (0.05\n",
    "             by default)\n",
    "        7. ooc_threshold : float\n",
    "            -Out of class threshold to see whether flagged\n",
    "             samples belong to other class(es) (0.95 by\n",
    "             default)\n",
    "        8. prob_method : str\n",
    "            -Method to use for calculating the likelihood,\n",
    "             either specify a distribution type or use a\n",
    "             non-parametric estimator (\"gaussian\" is\n",
    "             default)\n",
    "        9. data_transforms : func\n",
    "            -Any data transformations performed prior to use\n",
    "             by the model # specify torchvision transforms (NR)\n",
    "\n",
    "        ### Returns\n",
    "        1. label_issues_df : Pandas DataFrame (pd.DataFrame)\n",
    "            -A DataFrame containing the samples that are\n",
    "             suspected label errors and their associated\n",
    "             probabilities of belonging to each class,\n",
    "             as calculated by the above methods\n",
    "\n",
    "        ### Raises\n",
    "        ______\n",
    "        We'll get to that\n",
    "\n",
    "        ### object attributes created:\n",
    "        - self.conv_net\n",
    "        - self.X_\n",
    "        - self.y_\n",
    "        - self.classes_\n",
    "        - self.pca\n",
    "        - self.num_pca_components\n",
    "        - self.pca_explained_variance\n",
    "        - self.X_pca_tf\n",
    "        - self.c\n",
    "        - self.prediction_stats\n",
    "\n",
    "        \"\"\"\n",
    "        # Define Adaptive Labeling Error Detection (ALED) feature extraction model\n",
    "        # I changed \"ALED_model\" to \"self.conv_net\"; please don't kill me; I think CNN, conv_net, or FEM are better names (NR)\n",
    "        \n",
    "        self.conv_net = nn.Sequential(*list(copy.deepcopy(model).eval().children())[:-1])\n",
    "        # self.conv_net\n",
    "        # I think this^ won't work for all models; maybe we do a deep recursive dive into model.modules() or\n",
    "        #    model.children() to identify the last layer of type conv (NR)\n",
    "\n",
    "        self.conv_net.avgpool = torch.nn.AdaptiveAvgPool2d((1,1)) # Feature pooling of feature maps, probably will need to add another method for 1d and 3d features\n",
    "        print(self.conv_net)\n",
    "        # Create sorted representation of data (right now only works on images in chest x-ray dataset):\n",
    "\n",
    "        BATCH_SIZE = batch_size # maybe a param?\n",
    "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        feat_array = None # will be initialized once we know size of\n",
    "        y = np.zeros(len(dataset))\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (inputs, labels) in enumerate(dataloader):\n",
    "                # note: the dataset is responsible for holding any transformation functions;\n",
    "                # the transformations need to be the same transformations done for model evaluation; data augmentation should NOT be used!!!\n",
    "\n",
    "                batch_cuda = inputs.to(device)\n",
    "\n",
    "                batch_feat = self.conv_net(batch_cuda)\n",
    "                batch_feat_cpu = batch_feat.to('cpu').squeeze() # this might need to be changed - can we assume user wants to use 'cpu'?\n",
    "                # return batch_feat_cpu\n",
    "                if feat_array is None:\n",
    "                    feat_array = np.zeros((len(dataset), *batch_feat_cpu.shape[1:]))\n",
    "\n",
    "                feat_array[batch_num*BATCH_SIZE : batch_num*BATCH_SIZE + len(batch_feat_cpu)] = batch_feat_cpu\n",
    "                y[         batch_num*BATCH_SIZE : batch_num*BATCH_SIZE + len(batch_feat_cpu)] = labels\n",
    "\n",
    "                del batch_cuda\n",
    "                del batch_feat\n",
    "\n",
    "        # feat_array.squeeze()\n",
    "\n",
    "        print(\"feat_array.shape, y.shape:\", feat_array.shape, y.shape)\n",
    "        # Check that X and y have correct shape\n",
    "        feat_array, y = check_X_y(feat_array, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = feat_array\n",
    "        self.y_ = y\n",
    "#         self.sample_weights = sample_weight # might need sample_weight param for compatibility with sklearn - document if ignored\n",
    "\n",
    "        # perform PCA\n",
    "        from sklearn.decomposition import PCA\n",
    "        self.pca = PCA(n_components = max_pca_components) # don't rename this - we overwrite this variable 3 lines later\n",
    "        self.pca.fit(feat_array)\n",
    "        calc_var, max_variance_num_comps = self.sum_until(self.pca.explained_variance_ratio_, max_pca_variance)\n",
    "        self.pca = PCA(n_components = min(max_pca_components, max_variance_num_comps)).fit(feat_array)\n",
    "        self.X_pca_tf = self.pca.transform(feat_array) # pca_tf means pca transformed\n",
    "\n",
    "        # instead of PCA, consider sklearn.random_projection.GaussianRandomProjection - maybe more robust to noise (NR)\n",
    "        # also, random_projection.johnson_lindenstrauss_min_dim(...) finds a 'safe' number of components to randomly project to.\n",
    "\n",
    "\n",
    "        # optional (could be helpful for user):\n",
    "        self.num_pca_components = self.pca.n_components_\n",
    "        self.pca_explained_variance = calc_var\n",
    "        print(\"number of pca components:\", self.num_pca_components)\n",
    "        print(\"PCA explained variance:\", self.pca_explained_variance)\n",
    "\n",
    "        # Calculate covariance matrices for all class distributions\n",
    "        self.num_classes = len(self.classes_)\n",
    "        self.class_indices_dict = {}\n",
    "        self.priors_dict = {}\n",
    "        self.PC_dict = {}\n",
    "        self.cov_dict = {}\n",
    "        self.mvn_distributions = {}\n",
    "\n",
    "        for class_i in self.classes_:\n",
    "            class_indices = np.array(np.arange(len(feat_array))[y == class_i])\n",
    "            self.class_indices_dict[\"class{}_indices\".format(class_i)] = class_indices\n",
    "            class_PC_array = self.X_pca_tf[class_indices, :]\n",
    "            self.PC_dict[\"class{}_PC_array\".format(class_i)] = class_PC_array\n",
    "            self.cov_dict[\"robust_cov{}\".format(class_i)] = MinCovDet(random_state=self.random_state).fit(class_PC_array)\n",
    "            # self.mvn_distributions[\"mvn_{}\".format(class_i)] = scipy.stats.multivariate_normal(mean=class_PC_array.mean(axis=0), cov=np.cov(class_PC_array, rowvar=False), seed=self.random_state) # this line from chatgpt, so could be good to double check the cov matrix is right (NR)\n",
    "\n",
    "        # Calculate prior probabilities\n",
    "        for class_i in self.classes_:\n",
    "            prior = len(self.class_indices_dict[\"class{}_indices\".format(class_i)]) / len(y)\n",
    "            self.priors_dict[\"prior_prob{}\".format(class_i)] = prior\n",
    "\n",
    "        # Calculate likelihoods\n",
    "        likelihoods_df_dict = {}\n",
    "        for class_i in self.classes_:\n",
    "            likelihoods_dict = {}\n",
    "            for class_j in self.classes_:\n",
    "                cov_matrix = self.cov_dict[\"robust_cov{}\".format(class_j)].covariance_ #raw_covariance_ # why do we use raw_covariance_ instead of covariance_? (NR)\n",
    "                cov_mahalanobis = self.cov_dict[\"robust_cov{}\".format(class_j)].mahalanobis(self.PC_dict[\"class{}_PC_array\".format(class_i)]) # I think this was redundant; see MinCovDet.dist_ (NR)\n",
    "                likelihoods_dict[\"p(x | k = {})\".format(class_j)] = np.array([self.gaussian_likelihood(cov_matrix, sample) for sample in cov_mahalanobis])\n",
    "\n",
    "                # mahalanobis_distances = self.cov_dict[\"robust_cov{}\".format(class_j)].dist_\n",
    "                # likelihoods_dict[\"p(x | k = {})\".format(class_j)] = np.array([self.gaussian_likelihood(cov_matrix, sample**2) for sample in mahalanobis_distances])\n",
    "\n",
    "                # likelihoods = self.mvn_distributions[\"mvn_{}\".format(class_j)].pdf(self.PC_dict[\"class{}_PC_array\".format(class_i)])\n",
    "                # class_j_pca_tf = self.PC_dict[\"class{}_PC_array\".format(class_j)]\n",
    "                # likelihoods = scipy.stats.multivariate_normal.pdf(self.PC_dict[\"class{}_PC_array\".format(class_i)], mean=class_j_pca_tf.mean(axis=0), cov=np.cov(class_j_pca_tf, rowvar=False))\n",
    "                # likelihoods_dict[\"p(x | k = {})\".format(class_j)] = likelihoods\n",
    "\n",
    "            likelihoods_df = pd.DataFrame.from_dict(likelihoods_dict).set_index(self.class_indices_dict[\"class{}_indices\".format(class_i)])\n",
    "            likelihoods_df_dict[\"likelihood_df{}\".format(class_i)] = likelihoods_df\n",
    "\n",
    "        # Calculate probabilities\n",
    "        final_dfs_list = []\n",
    "        for class_i in self.classes_:\n",
    "            likelihoods_df = likelihoods_df_dict[\"likelihood_df{}\".format(class_i)]\n",
    "            for class_j in self.classes_:\n",
    "                likelihoods_df[\"p(k = {} | x)\".format(class_j)] = likelihoods_df[\"p(x | k = {})\".format(class_j)] * self.priors_dict[\"prior_prob{}\".format(class_j)] / \\\n",
    "                    sum( [likelihoods_df[\"p(x | k = {})\".format(class_k)] * self.priors_dict[\"prior_prob{}\".format(class_k)] for class_k in self.classes_] )\n",
    "#             likelihoods_df[\"given label (num)\"] = n\n",
    "            likelihoods_df[\"ooc prob\"] = likelihoods_df.iloc[:,len(self.classes_):].drop(\"p(k = {} | x)\".format(class_i), axis=1).sum(axis=1)\n",
    "            likelihoods_df[\"given label (name)\"] = class_i\n",
    "            # likelihoods_df[\"updated label (name)\"] =\n",
    "            final_dfs_list.append(likelihoods_df)\n",
    "\n",
    "        self.prediction_stats = pd.concat(final_dfs_list).sort_index()\n",
    "        self.likelihood_ratio_threshold = likelihood_ratio_threshold\n",
    "        self.prediction_stats['Aled label'] = self.prediction_stats.apply(self.extract_out_label, axis=1)\n",
    "\n",
    "        return self.prediction_stats['Aled label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU-Conserving Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_until(in_list, threshold):\n",
    "    \"\"\"\n",
    "    Helper function for find_label_issues\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for n in range(len(in_list)):\n",
    "        if count < threshold:\n",
    "            count += in_list[n]\n",
    "        else:\n",
    "            break\n",
    "    return count, n+1\n",
    "\n",
    "def gaussian_likelihood(cov_matrix, mahalanobis_square_dist):\n",
    "    \"\"\"\n",
    "    Helper function for find_label_issues\n",
    "    \"\"\"\n",
    "    n = cov_matrix.shape[0]\n",
    "    sqrt_det = np.sqrt(np.linalg.det(cov_matrix))\n",
    "    prob_x_f = ( 1 / ( (2*np.pi)**(n/2) * sqrt_det ) ) * np.exp(-mahalanobis_square_dist)\n",
    "    return prob_x_f\n",
    "\n",
    "def find_label_issues(model, dataset, device=None, max_pca_components=10, max_pca_variance=0.25, likelihood_ratio_threshold=2, prob_method=\"gaussian\", batch_size=4):\n",
    "    \"\"\"\n",
    "    Creates a new model that aggregates input classification model features / feature maps at last layer before classification into a [p x 1] vector. \n",
    "    Then performs PCA to generate an [n x 1] vector of PCs, where n is specified either explicitly or by a desired level of explained variance. Then\n",
    "    the probability that a given sample belongs to the assigned class is assessed using Bayes' rule, and if the probability falls below the within \n",
    "    class threshold, then the sample is compared to the other class(es), and if the probability that it belongs in another class is higher than the \n",
    "    out of class threshold, then the sample is added to an output DataFrame containing suspect samples and their associated probabilities. This\n",
    "    function is dependent on the ability of the input classification model to extract salient features from the input data and thus should only be\n",
    "    applied if the model is achieving some threshold auc / accuracy. \n",
    "\n",
    "    ### Parameters\n",
    "    1. model : Pytorch model (torch.nn.Sequential)\n",
    "        -A classification model\n",
    "    2. data : Pytorch Dataset (torch.utils.data.Dataset, eventually...right now needs an ImageFolder)\n",
    "        -The dataset used to train model\n",
    "    3. pca_method : str\n",
    "        -Use this string to specify whether to\n",
    "         use the first n principal components \n",
    "         or to use as many components as needed\n",
    "         to achieve a target explained variance\n",
    "         (\"variance\" for variance method (default)\n",
    "         or \"components\" for components method)\n",
    "    4. pca_components : int\n",
    "        -Number of components if using component\n",
    "         method (None by default)\n",
    "    5. pca_variance : float\n",
    "        -Ratio of explained variance desired, if\n",
    "         using explained variance method (0.25 by\n",
    "         default)\n",
    "    6. wc_threshold : float\n",
    "        -Within class threshold to consider sample\n",
    "         within distribution of other class(es) (0.05\n",
    "         by default)\n",
    "    7. ooc_threshold : float\n",
    "        -Out of class threshold to see whether flagged\n",
    "         samples belong to other class(es) (0.95 by\n",
    "         default)\n",
    "    8. prob_method : str\n",
    "        -Method to use for calculating the likelihood,\n",
    "         either specify a distribution type or use a \n",
    "         non-parametric estimator (\"gaussian\" is \n",
    "         default)\n",
    "    9. data_transforms : func\n",
    "        -Any data transformations performed prior to use \n",
    "         by the model\n",
    "    \n",
    "    ### Returns\n",
    "    1. label_issues_df : Pandas DataFrame (pd.DataFrame)\n",
    "        -A DataFrame containing the samples that are\n",
    "         suspected label errors and their associated\n",
    "         probabilities of belonging to each class, \n",
    "         as calculated by the above methods\n",
    "    \n",
    "    Raises\n",
    "    \n",
    "    We'll get to that\n",
    "\n",
    "    \"\"\"\n",
    "    # Define Adaptive Labeling Error Detection (ALED) feature extraction model\n",
    "    ALED_model = nn.Sequential(*list(model.children())[:-1])\n",
    "    ALED_model.avgpool = torch.nn.AdaptiveAvgPool2d((1,1)) # Feature pooling of feature maps, probably will need to add another method for 1d and 3d features\n",
    "\n",
    "    BATCH_SIZE = batch_size # maybe a param?\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    feat_array = None # will be initialized once we know size of\n",
    "    y = np.zeros(len(dataset))\n",
    "\n",
    "    for batch_num, (inputs, labels) in enumerate(dataloader):\n",
    "        # note: the dataset is responsible for holding any transformation functions;\n",
    "        # the transformations need to be the same transformations done for model evaluation; data augmentation should NOT be used!!!\n",
    "\n",
    "        batch_cuda = inputs.to(device)\n",
    "\n",
    "        batch_feat = ALED_model(batch_cuda)\n",
    "        batch_feat_cpu = batch_feat.to('cpu').squeeze() # this might need to be changed - can we assume user wants to use 'cpu'?\n",
    "        # return batch_feat_cpu\n",
    "        if feat_array is None:\n",
    "            feat_array = np.zeros((len(dataset), *batch_feat_cpu.shape[1:]))\n",
    "\n",
    "        feat_array[batch_num*BATCH_SIZE : batch_num*BATCH_SIZE + len(batch_feat_cpu)] = batch_feat_cpu\n",
    "        y[         batch_num*BATCH_SIZE : batch_num*BATCH_SIZE + len(batch_feat_cpu)] = labels\n",
    "\n",
    "        del batch_cuda\n",
    "        del batch_feat\n",
    "\n",
    "    # Store the classes seen during fit\n",
    "    classes_ = unique_labels(y)\n",
    "\n",
    "    X_ = feat_array\n",
    "    y_ = y\n",
    "#         self.sample_weights = sample_weight # might need sample_weight param for compatibility with sklearn - document if ignored\n",
    "\n",
    "    # perform PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = max_pca_components) # don't rename this - we overwrite this variable 3 lines later\n",
    "    pca.fit(feat_array)\n",
    "    calc_var, max_variance_num_comps = sum_until(pca.explained_variance_ratio_, max_pca_variance)\n",
    "    pca = PCA(n_components = min(max_pca_components, max_variance_num_comps)).fit(feat_array)\n",
    "    X_pca_tf = pca.transform(feat_array) # pca_tf means pca transformed\n",
    "\n",
    "    # instead of PCA, consider sklearn.random_projection.GaussianRandomProjection - maybe more robust to noise (NR)\n",
    "    # also, random_projection.johnson_lindenstrauss_min_dim(...) finds a 'safe' number of components to randomly project to.\n",
    "\n",
    "\n",
    "    # optional (could be helpful for user):\n",
    "    num_pca_components = pca.n_components_\n",
    "    pca_explained_variance = calc_var\n",
    "    print(\"number of pca components:\", num_pca_components)\n",
    "    print(\"PCA explained variance:\", pca_explained_variance)\n",
    "\n",
    "    # Calculate covariance matrices for all class distributions\n",
    "    num_classes = len(classes_)\n",
    "    class_indices_dict = {}\n",
    "    priors_dict = {}\n",
    "    PC_dict = {}\n",
    "    cov_dict = {}\n",
    "    mvn_distributions = {}\n",
    "\n",
    "    for class_i in classes_:\n",
    "        class_indices = np.array(np.arange(len(feat_array))[y == class_i])\n",
    "        class_indices_dict[\"class{}_indices\".format(class_i)] = class_indices\n",
    "        class_PC_array = X_pca_tf[class_indices, :]\n",
    "        PC_dict[\"class{}_PC_array\".format(class_i)] = class_PC_array\n",
    "        cov_dict[\"robust_cov{}\".format(class_i)] = MinCovDet(random_state=0).fit(class_PC_array)\n",
    "        # self.mvn_distributions[\"mvn_{}\".format(class_i)] = scipy.stats.multivariate_normal(mean=class_PC_array.mean(axis=0), cov=np.cov(class_PC_array, rowvar=False), seed=self.random_state) # this line from chatgpt, so could be good to double check the cov matrix is right (NR)\n",
    "\n",
    "    # Calculate prior probabilities\n",
    "    for class_i in classes_:\n",
    "        prior = len(class_indices_dict[\"class{}_indices\".format(class_i)]) / len(y)\n",
    "        priors_dict[\"prior_prob{}\".format(class_i)] = prior\n",
    "\n",
    "    # Calculate likelihoods\n",
    "    likelihoods_df_dict = {}\n",
    "    for class_i in classes_:\n",
    "        likelihoods_dict = {}\n",
    "        for class_j in classes_:\n",
    "            cov_matrix = cov_dict[\"robust_cov{}\".format(class_j)].covariance_ #raw_covariance_ # why do we use raw_covariance_ instead of covariance_? (NR)\n",
    "            cov_mahalanobis = cov_dict[\"robust_cov{}\".format(class_j)].mahalanobis(PC_dict[\"class{}_PC_array\".format(class_i)]) # I think this was redundant; see MinCovDet.dist_ (NR)\n",
    "            likelihoods_dict[\"p(x | k = {})\".format(class_j)] = np.array([gaussian_likelihood(cov_matrix, sample) for sample in cov_mahalanobis])\n",
    "\n",
    "        likelihoods_df = pd.DataFrame.from_dict(likelihoods_dict).set_index(class_indices_dict[\"class{}_indices\".format(class_i)])\n",
    "        likelihoods_df_dict[\"likelihood_df{}\".format(class_i)] = likelihoods_df\n",
    "\n",
    "    # Calculate probabilities\n",
    "    final_dfs_list = []\n",
    "    for class_i in classes_:\n",
    "        likelihoods_df = likelihoods_df_dict[\"likelihood_df{}\".format(class_i)]\n",
    "        for class_j in classes_:\n",
    "            likelihoods_df[\"p(k = {} | x)\".format(class_j)] = likelihoods_df[\"p(x | k = {})\".format(class_j)] * priors_dict[\"prior_prob{}\".format(class_j)] / \\\n",
    "                sum( [likelihoods_df[\"p(x | k = {})\".format(class_k)] * priors_dict[\"prior_prob{}\".format(class_k)] for class_k in classes_] )\n",
    "        likelihoods_df[\"ooc prob\"] = likelihoods_df.iloc[:,len(classes_):].drop(\"p(k = {} | x)\".format(class_i), axis=1).sum(axis=1)\n",
    "        likelihoods_df[\"given label (name)\"] = class_i\n",
    "        final_dfs_list.append(likelihoods_df)\n",
    "\n",
    "    def extract_out_label(prediction_stats_row):\n",
    "        given_label = prediction_stats_row['given label (name)']\n",
    "        probabilities_row = np.array([prediction_stats_row[\"p(k = {} | x)\".format(class_i)] for class_i in classes_])\n",
    "        if (classes_[probabilities_row.argmax()] != given_label)\\\n",
    "            and (probabilities_row.max()/prediction_stats_row[\"p(k = {} | x)\".format(given_label)] > likelihood_ratio_threshold):\n",
    "            return classes_[probabilities_row.argmax()]\n",
    "        else:\n",
    "            return given_label\n",
    "        \n",
    "    prediction_stats = pd.concat(final_dfs_list).sort_index()\n",
    "    likelihood_ratio_threshold = likelihood_ratio_threshold\n",
    "    prediction_stats['Aled label'] = prediction_stats.apply(extract_out_label, axis=1)\n",
    "\n",
    "    return prediction_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def generate(model, dataset, BATCH_SIZE):\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Generate outputs from random model\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images_cuda = images.to(device)\n",
    "\n",
    "            output = model(images_cuda)\n",
    "\n",
    "            del images_cuda\n",
    "\n",
    "            yield output\n",
    "\n",
    "def create_dataset_labels(outputs_generator):\n",
    "    outputs_list = []\n",
    "    for output in outputs_generator:\n",
    "        output_cpu = output.cpu()\n",
    "        outputs_list.append(output_cpu)\n",
    "        del output\n",
    "    outputs_tensor = torch.cat(outputs_list).squeeze()\n",
    "    return outputs_tensor\n",
    "\n",
    "def softmaxed_outputs_array(model, dataset, BATCH_SIZE):\n",
    "    model_outputs = create_dataset_labels(generate(model, dataset, BATCH_SIZE))\n",
    "    m = nn.Softmax(dim=1)\n",
    "    softmaxed_array = m(model_outputs).numpy()\n",
    "    return softmaxed_array\n",
    "\n",
    "def CL_num_estimate(model, dataset, BATCH_SIZE):\n",
    "  outs = softmaxed_outputs_array(model, dataset, BATCH_SIZE)\n",
    "  thresholds_dict = {}\n",
    "  for n in range(len(dataset.classes)):\n",
    "      idx_n = np.where(np.array(dataset.labels) == n)[0]\n",
    "      thresh_n = np.mean(outs[idx_n, n])\n",
    "      thresholds_dict[n] = thresh_n\n",
    "\n",
    "  predictions = np.argmax(outs, axis=1)\n",
    "  df = pd.DataFrame(outs)\n",
    "  df[\"label\"] = np.array(dataset.labels)\n",
    "  df[\"pred\"] = predictions\n",
    "\n",
    "  misclass_df = df[df[\"pred\"] != df[\"label\"]]\n",
    "  misclass_df[\"CL_pred\"] = misclass_df.apply(lambda row : row[row[\"pred\"]] > thresholds_dict[row[\"pred\"]], axis=1)\n",
    "\n",
    "  num_estimate = sum(misclass_df[\"CL_pred\"])\n",
    "\n",
    "  return misclass_df, num_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(num_epochs, cnn, loaders, optimizer, loss_func):\n",
    "\n",
    "    cnn.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images).to(device)   # batch x\n",
    "            b_y = Variable(labels).to(device)   # batch y\n",
    "            output = cnn(b_x)\n",
    "            loss = loss_func(output, b_y)\n",
    "\n",
    "            # clear gradients for this training step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation, compute gradients\n",
    "            loss.backward()\n",
    "            # apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Mislabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class Mislabeling_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    a Dataset class that contains a Dataset, providing the data of the internal Dataset\n",
    "    but with labeling errors\n",
    "    \"\"\"\n",
    "    def __init__(self, internal_dataset, internal_dataset_labels = None, fraction_mislabeled = 0.1, random_state = None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.internal_dataset = copy.deepcopy(internal_dataset)\n",
    "        self.true_labels = copy.deepcopy(internal_dataset_labels)\n",
    "        if self.true_labels is None:\n",
    "            self.set_true_labels()\n",
    "        elif len(self.true_labels) != len(internal_dataset):\n",
    "          raise Exception(\"Must have same number of labels as the length of the dataset [i.e. len(internal_dataset_labels) == len(internal_dataset)]\")\n",
    "        self.classes = unique_labels(self.true_labels)\n",
    "        self.fraction_mislabeled = fraction_mislabeled\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        # could potentially add a parameter to choose what method to mislabel (e.g., completely random, half mislabeled from each class)\n",
    "\n",
    "\n",
    "        self.mislabel_sample_inds = self.rng.choice(np.arange(len(self.internal_dataset)), size=int(self.fraction_mislabeled*len(internal_dataset)), replace=False)\n",
    "        mislabel_maps = [[class_j for class_j in self.classes if class_j != class_i] for class_i in self.classes]\n",
    "\n",
    "        self.labels = copy.deepcopy(self.true_labels)\n",
    "        for ind in self.mislabel_sample_inds:\n",
    "            self.labels[ind] = mislabel_maps[self.true_labels[ind]][self.rng.integers(len(self.classes)-1)]\n",
    "\n",
    "\n",
    "    def set_true_labels(self):\n",
    "        # helper function used by __init__()\n",
    "        self.true_labels = np.zeros(len(self.internal_dataset), dtype=object)\n",
    "        for i in range(len(self.internal_dataset)):\n",
    "            self.true_labels[i] = self.internal_dataset[i][1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return self.internal_dataset[idx][0], self.labels[idx].item()\n",
    "        except:\n",
    "            return self.internal_dataset[idx][0], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.internal_dataset)\n",
    "\n",
    "class custom_Subset(Subset):\n",
    "  def __init__(self, dataset, indices):\n",
    "    self.classes = dataset.classes\n",
    "    self.indices = indices\n",
    "    self.dataset = dataset\n",
    "    self.labels = dataset.labels[indices]\n",
    "    self.true_labels = dataset.true_labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALED Looping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "from torch import optim\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def ALED_Looper(cnn, mislabeled_data, num_epochs, num_ALED, batch_size, initial_train=True, num_est=None):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr = 0.01)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if initial_train:\n",
    "        loaders = {\n",
    "            'train' : torch.utils.data.DataLoader(mislabeled_data,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "        }\n",
    "\n",
    "        train(num_epochs, cnn, loaders, optimizer, loss_func)\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        m_df, num_est = CL_num_estimate(cnn, mislabeled_data, batch_size)\n",
    "\n",
    "        warnings.filterwarnings(\"always\")\n",
    "\n",
    "    for n in range(num_ALED):\n",
    "        print(\"ALED Starting\")\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        with HiddenPrints():\n",
    "            aled = ALED()\n",
    "            prob_df = aled.fit_predict(model=cnn, dataset=mislabeled_data, device=device, batch_size=1000, max_pca_variance=0.99, max_pca_components=8)\n",
    "\n",
    "        warnings.filterwarnings(\"always\")\n",
    "\n",
    "        posterior_probs_df = aled.prediction_stats.iloc[:,10:]\n",
    "        indices_to_remove = posterior_probs_df.sort_values(by=[\"ooc prob\"], axis=0, ascending=False).head(num_est).index.to_numpy()\n",
    "        indices_to_keep = [i for i in posterior_probs_df.index.to_numpy() if i not in indices_to_remove]\n",
    "\n",
    "        filtered_training_data = custom_Subset(mislabeled_data, indices_to_keep)\n",
    "        removed_training_data = custom_Subset(mislabeled_data, indices_to_remove)\n",
    "\n",
    "        posterior_probs_df[\"true labels\"] = mislabeled_data.true_labels\n",
    "        checking_df = posterior_probs_df.sort_values(by=[\"ooc prob\"], axis=0, ascending=False).head(num_est)\n",
    "        num_correct = len(checking_df[checking_df[\"true labels\"] != checking_df[\"given label (name)\"]])\n",
    "        print(\"Num Est:\", num_est)\n",
    "        print(\"Num Correct:\", num_correct)\n",
    "        print(\"Percent Correct: \", round(100*num_correct/num_est, 2))\n",
    "\n",
    "        print(\"ALED Done\")\n",
    "        loaders_ALED = {\n",
    "            'train' : torch.utils.data.DataLoader(filtered_training_data,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0)\n",
    "        }\n",
    "\n",
    "        train(num_epochs, cnn, loaders_ALED, optimizer, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CL_Looper(cnn, mislabeled_data, num_epochs, batch_size):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr = 0.01)\n",
    "\n",
    "    loaders = {\n",
    "        'train' : torch.utils.data.DataLoader(mislabeled_data,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=0)\n",
    "    }\n",
    "\n",
    "    train(num_epochs, cnn, loaders, optimizer, loss_func)\n",
    "\n",
    "    print(\"Starting Cleanlab\")\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    m_df, num_est = CL_num_estimate(cnn, mislabeled_data, batch_size)\n",
    "\n",
    "    warnings.filterwarnings(\"always\")\n",
    "\n",
    "    CL_array = m_df[m_df[\"CL_pred\"]==True].index.to_numpy()\n",
    "    truth_array = np.equal(np.array(mislabeled_data.true_labels), np.array(mislabeled_data.labels))\n",
    "    truth_array = np.where(truth_array==False)\n",
    "    num_correct = sum(np.in1d(CL_array, truth_array))\n",
    "    print(\"Num Est:\", num_est)\n",
    "    print(\"Num Correct:\", num_correct)\n",
    "    print(\"Percent Correct: \", round(100*num_correct/num_est, 2))\n",
    "\n",
    "    return num_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "data_T = T.Compose([\n",
    "                \n",
    "                T.Resize(size = (224,224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "trainset = datasets.ImageFolder(data_dir, transform = data_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torchvision.models.densenet161(weights=\"DenseNet161_Weights.DEFAULT\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(trainset.classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_train_data = Mislabeling_Dataset(trainset, internal_dataset_labels=trainset.targets, fraction_mislabeled=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/1304], Loss: 0.0149\n",
      "Epoch [1/2], Step [200/1304], Loss: 0.0000\n",
      "Epoch [1/2], Step [300/1304], Loss: 0.0268\n",
      "Epoch [1/2], Step [400/1304], Loss: 0.0097\n",
      "Epoch [1/2], Step [500/1304], Loss: 0.6661\n",
      "Epoch [1/2], Step [600/1304], Loss: 0.0013\n",
      "Epoch [1/2], Step [700/1304], Loss: 1.4871\n",
      "Epoch [1/2], Step [800/1304], Loss: 0.0035\n",
      "Epoch [1/2], Step [900/1304], Loss: 2.8204\n",
      "Epoch [1/2], Step [1000/1304], Loss: 0.6670\n",
      "Epoch [1/2], Step [1100/1304], Loss: 0.0000\n",
      "Epoch [1/2], Step [1200/1304], Loss: 0.0001\n",
      "Epoch [1/2], Step [1300/1304], Loss: 3.2462\n",
      "Epoch [2/2], Step [100/1304], Loss: 0.0105\n",
      "Epoch [2/2], Step [200/1304], Loss: 0.0000\n",
      "Epoch [2/2], Step [300/1304], Loss: 0.0236\n",
      "Epoch [2/2], Step [400/1304], Loss: 0.0001\n",
      "Epoch [2/2], Step [500/1304], Loss: 2.0114\n",
      "Epoch [2/2], Step [600/1304], Loss: 0.0009\n",
      "Epoch [2/2], Step [700/1304], Loss: 1.3286\n",
      "Epoch [2/2], Step [800/1304], Loss: 0.0005\n",
      "Epoch [2/2], Step [900/1304], Loss: 2.1368\n",
      "Epoch [2/2], Step [1000/1304], Loss: 2.0623\n",
      "Epoch [2/2], Step [1100/1304], Loss: 0.0000\n",
      "Epoch [2/2], Step [1200/1304], Loss: 0.0000\n",
      "Epoch [2/2], Step [1300/1304], Loss: 1.1216\n",
      "Starting Cleanlab\n",
      "Num Est: 1623\n",
      "Num Correct: 233\n",
      "Percent Correct:  14.36\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "\n",
    "num_est = CL_Looper(cnn=model, mislabeled_data=mislabeled_train_data, num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALED Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pca components: 10\n",
      "PCA explained variance: 0.24977704547712423\n",
      "Num Est: 1623\n",
      "Num Correct: 496\n",
      "Percent Correct:  30.56\n"
     ]
    }
   ],
   "source": [
    "num_ALED = 1\n",
    "\n",
    "prob_df = find_label_issues(model, mislabeled_train_data, device=device, batch_size=batch_size)\n",
    "prob_df[\"true labels\"] = mislabeled_train_data.true_labels\n",
    "checking_df = prob_df.sort_values(by=[\"ooc prob\"], axis=0, ascending=False).head(num_est)\n",
    "num_correct = len(checking_df[checking_df[\"true labels\"] != checking_df[\"given label (name)\"]])\n",
    "print(\"Num Est:\", num_est)\n",
    "print(\"Num Correct:\", num_correct)\n",
    "print(\"Percent Correct:\", round(100*num_correct/num_est, 2))\n",
    "\n",
    "#output = ALED_Looper(cnn=model, mislabeled_data=mislabeled_train_data, num_epochs=num_epochs, num_ALED=num_ALED, batch_size=batch_size, initial_train=False, num_est=num_est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
